% !TEX root = ../linal_lecture_06.tex

\begin{frame} % название фрагмента

\videotitle{PCA: максимизация разброса}

\end{frame}



\begin{frame}{Краткий план:}
  \begin{itemize}[<+->]
    \item Максимизация выборочной дисперсии.
  \item Свойства главных компонент.
    \end{itemize}

\end{frame}




\begin{frame}
  \frametitle{Метод главных компонент}


  Есть матрица $X$ исходных наблюдений:

  наблюдения отложены по строкам, 
  
  а переменные — по столбцам. \pause


  Переменных очень много. \pause


%  А мы хотим визуализировать данные. \pause

  Хотим иметь небольшое количество переменных, 
  которые бы почти без потерь 
  содержали всю информацию об исходных переменных. 


\end{frame}


\begin{frame}
  \frametitle{Метод главных компонент}

  Все исходные переменные предварительно стандартизируем! \pause

  Для каждого столбца $\bx$ выполнены условия $\bar x =0$, $sd(\bx)=1$. \pause


  На базе столбцов $\bx_1$, $\bx_2$, \ldots, $\bx_k$ матрицы $X$ мы создадим 
  $d\leq k$ новых переменных $\bp_1$, $\bp_2$, \ldots, $\bp_d$. \pause

  Новые переменные будем создавать по-очереди. \pause

  Новые переменные будем называть \alert{главными компонентами}.

  PCA — principal component analysis.

\end{frame}


\begin{frame}
  \frametitle{Максимизация разброса}

  Главные компоненты $\bp_1$, $\bp_2$, \ldots, $\bp_d$ будут линейными 
  комбинациями столбцов $X$. \pause

  \begin{block}{Алгоритм}
    \begin{enumerate}
      \item Компоненту $\bp_1 = X \bv_1$ подберём так, чтобы 
      выборочная дисперсия $\bp_1$ была максимальной при условии, что $\norm{\bv_1}=1$. \pause
    \item Компоненту $\bp_2 = X \bv_2$ подберём так, чтобы 
    выборочная дисперсия $\bp_2$ была максимальной при условии, что $\bv_2 \perp \bv_1$ и $\norm{\bv_2}=1$. \pause
  \item Компоненту $\bp_3 = X \bv_3$ подберём так, чтобы 
  выборочная дисперсия $\bp_3$ была максимальной при условии, что $\bv_3 \perp \bv_2$, $\bv_3 \perp \bv_1$ и $\norm{\bv_3}=1$. 
      \item \ldots
    \end{enumerate}
    
  \end{block}
  

\end{frame}


\begin{frame}
  \frametitle{Картинка}


\begin{tikzpicture}[
  scale=1.4,
  MyPoints/.style={draw=blue,fill=white,thick},
  Segments/.style={draw=blue!50!red!70,thick},
  MyCircles/.style={green!50!blue!50,thin}, 
  every node/.style={scale=1}
  ]
%	\draw[color=gray,step=1.0,dotted] (-8,-6) grid (8,6);
  \clip (-8,-6) rectangle (8,6);


  %\draw[->, >=stealth] (-1,0)--(6.5,0) node[right]{$x_1$};
  \draw[-{Latex[length=4.5mm, width=2.5mm]}, >=stealth] (0,-5)--(0,5) node[above left]{$x_2$};

  \draw[-{Latex[length=4.5mm, width=2.5mm]}, >=stealth] (-6,0)--(6,0) 
  node[right]{$x_1$};


  %{\verb!->!new, arrowhead = 2mm, line width=4pt}
  %, arrowhead = 3mm
  %, arrowhead = 0.2

  % Feel free to change here coordinates of points A and B
  \pgfmathparse{0}		\let\Xa\pgfmathresult
  \pgfmathparse{0}		\let\Ya\pgfmathresult
  \coordinate (A) at (\Xa,\Ya);

  \pgfmathparse{-1}		\let\Xb\pgfmathresult
  \pgfmathparse{1}		\let\Yb\pgfmathresult
  \coordinate (B) at (\Xb,\Yb);

  \pgfmathparse{1}		\let\Xc\pgfmathresult
  \pgfmathparse{1}		\let\Yc\pgfmathresult
  \coordinate (C) at (\Xc,\Yc);






  \begin{scope}
  \clip[rotate=45] (0, 0) ellipse (4 and 2);
  \foreach \p in {1,...,800}
  { \fill[black, rotate = 45]  (0 + 4*rand,3*rand) circle (0.035);
  }
  \end{scope}
  \pause


  \draw[line width = 0.5mm, ->, vecb] (-4, -4) -- (4, 4) node[above left]{$p_{1}$}; \pause


  \draw[line width = 0.5mm, ->, vecb] (3, -3) -- (-3, 3) node[above right]{$p_{2}$}; 
\tkzMarkRightAngle[size=0.3, vecb, line width = 0.3mm](B,A,C);


  \end{tikzpicture}


  

  

\end{frame}


\begin{frame}
  \frametitle{Это завуалированный $SVD$!}

  Если $X = U\Sigma V^T$, то $P = XV = U\Sigma$. \pause

  \[
    \begin{array}{l}
    \begin{pmatrix}
        \vert &  & \vert \\
        \bp_1 & \ldots & \bp_k \\
        \vert &    & \vert \\
      \end{pmatrix}  = 
      \begin{pmatrix}
        \vert &  & \vert \\
        \bx_1 & \ldots & \bx_k \\
        \vert &    & \vert \\
      \end{pmatrix}
      \begin{pmatrix}
        \vert &  & \vert \\
        \bv_1 & \ldots & \bv_k \\
        \vert &    & \vert \\
      \end{pmatrix} \\
      \begin{pmatrix}
        \vert &  & \vert \\
        \bp_1 & \ldots & \bp_k \\
        \vert &    & \vert \\
      \end{pmatrix}  = 
      \begin{pmatrix}
        \vert &  & \vert \\
        \bu_1 & \ldots & \bu_n \\
        \vert &    & \vert \\
      \end{pmatrix}
      \begin{pmatrix}
        \sigma_1 & 0 & \ldots & 0 \\
         0 & \sigma_2 & \ldots & 0 \\
        \ldots & \ldots & \ldots & \ldots \\
    0 & 0 & \ldots & \sigma_k \\
    0 & 0 & \ldots & 0 \\
    \ldots & \ldots & \ldots & \ldots \\
    0 & 0 & \ldots & 0 \\
      \end{pmatrix} 
    \end{array} \pause
  \]

  % Если вектор $\bu_1$ растянуть в $\sigma_1$ раз, то получится первая главная компонента $\bp_1$. \pause

  % Если вектор $\bu_2$ растянуть в $\sigma_2$ раз, то получится первая главная компонента $\bp_2$. 


%  \ldots

  

\end{frame}



\begin{frame}
  \frametitle{Свойства}

  Главные компоненты находятся из $SVD$ разложения, $\bp_i = \sigma_i \bu_i$. \pause

  Главные компоненты ортогональны. 

  Величины $\sigma^2_i$ равны квадратам длин $\norm{\bp_i}^2$ 
  и пропорциональны выборочным дисперсиям $\bp_i$.  \pause

Значение максимума суммы 
\[
  \norm{\bp_1}^2 + \ldots + \norm{\bp_d}^2 = \sigma^2_1 + \ldots + \sigma^2_d
\]
одинаково при пошаговой и одновременной максимизации.


\end{frame}


\begin{frame}
  \frametitle{Связь с корреляционной матрицей}
  Если $X = U\Sigma V^T$, то корреляционная матрица имеет вид $C=X^TX = V\Sigma^T \Sigma V^T$. \pause

  Векторы весов $\bv_i$, с которыми исходные переменные входят в компоненты, являются
  собственными векторами корреляционной матрицы $C$. \pause

  Сингулярные значения матрицы $X$ в квадрате являются собственными числами 
  корреляционной матрицы $C$, $\lambda_i = \sigma_i^2$.
  

\end{frame}