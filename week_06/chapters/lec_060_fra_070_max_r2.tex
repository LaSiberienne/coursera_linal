% !TEX root = ../linal_lecture_06.tex

\begin{frame} % название фрагмента

\videotitle{PCA: максимизация $R^2$}

\end{frame}



\begin{frame}{Краткий план:}
  \begin{itemize}[<+->]
    \item Коэффициент детерминации.
    \item Максимизация $R^2$.
    \item Свойства главных компонент.
  \end{itemize}

\end{frame}




\begin{frame}
  \frametitle{Коэффициент детерминации}

  
  Рассмотрим нахождение проекции $\bhy$ вектора $\by$ на
  линейную оболочку $\Span\{ \bx_1, \ldots, \bx_k\}$ столбцов матрицы $X$. \pause

  Предположим, что среди $\bx_i$ есть вектор-константа. \pause 

  \begin{block}{Определение}
    \alert{Коэффициентом детерминации $R^2$} называют квадрат косинуса угла 
    между центрированный вектором $\by'$, $y_i' = y_i - \bar y$,
    и его проекцией $\bhy'$ на столбцы $X$.
    \[
      R^2 = \cos^2 \angle(\bhy', \by') = \frac{\norm{\bhy'}^2}{\norm{\by'}^2} \pause
    \]
  \end{block}

  Коэффициент детерминации — доля «объяснённого разброса». 

\end{frame}

\begin{frame}
\frametitle{Коэффициент детерминации}

  \begin{block}{Утверждение}
  Коэффициент детерминации равен 
    квадрату выборочной корреляции между исходным $\by$ и его проекцией $\bhy$ на столбцы $X$:
    \[
      R^2 = \rho^2 (\bhy, \by) = \frac{\left( \sum_i (y_i - \bar y) (\hat y_i - \bar y) \right)^2}{\sum_i (y_i - \bar y)^2 \sum_i (\hat y_i - \bar y)^2 }
    \]
  \end{block}

\end{frame}




\begin{frame}
  \frametitle{Метод главных компонент}

  Все исходные переменные $\bx_1$, $\bx_2$, \ldots, $\bx_k$ предварительно стандартизируем! 

  Для каждого столбца $\bx$ выполнены условия $\bar x =0$, $sd(\bx)=1$. 


  На базе столбцов $\bx_1$, $\bx_2$, \ldots, $\bx_k$ матрицы $X$ мы создадим 
  $d\leq k$ новых переменных $\bp_1$, $\bp_2$, \ldots, $\bp_d$. 

  Новые переменные будем создавать по-очереди. 

  Новые переменные будем называть \alert{главными компонентами}.

  % PCA — \alert{principal component analysis}.

\end{frame}

\begin{frame}
  \frametitle{Обозначение}


  Обозначим $R^2_i(\bx)$ —
  коэффициент детерминации при проецировании вектора $\bx$ на 
  линейную оболочку первых~$i$~главных компонент, $\Span \{ \bp_1, \bp_2, \ldots, \bp_i  \}$. \pause


  Коэффициент $R^2_i(\bx)$ показывает, насколько хорошо первые~$i$~главных компонент
  предсказывают вектор $\bx$. 

\end{frame}


\begin{frame}
  \frametitle{Максимизация суммы $R^2$}

  Главные компоненты $\bp_1$, $\bp_2$, \ldots, $\bp_d$ будут линейными 
  комбинациями столбцов $X$. \pause

  \begin{block}{Алгоритм}
    \begin{enumerate}
      \item Компоненту $\bp_1 = X \bv_1$ подберём так, чтобы 
     максимизировать сумму $R_1^2(\bx_1) + \ldots + R_1^2(\bx_k)$ 
     при условии, что $\norm{\bv_1}=1$. \pause
\item Компоненту $\bp_2 = X \bv_2$ подберём так, чтобы 
максимизировать сумму $R_2^2(\bx_1) + \ldots + R_2^2(\bx_k)$ 
    при условии, что $\bv_2 \perp \bv_1$ и $\norm{\bv_2}=1$. 
%  \item Компоненту $\bp_3 = X \bv_3$ подберём так, чтобы 
 % выборочная дисперсия $\bp_3$ была максимальной при условии, что $\bv_3 \perp \bv_2,\, \bv_1$ и $\norm{\bv_3}=1$. 
      \item \ldots
    \end{enumerate}
    
  \end{block}
  

\end{frame}



\begin{frame}
  \frametitle{И это завуалированный $SVD$!}

  Если $X = U\Sigma V^T$, то $P = XV = U\Sigma$.

  Главные компоненты находятся из $SVD$ разложения, $\bp_i = \sigma_i \bu_i$. 

  Главные компоненты ортогональны. 

  Величины $\sigma^2_i$ равны квадратам длин $\norm{\bp_i}^2$ 
  и пропорциональны выборочным дисперсиям $\bp_i$.  \pause

Значение суммы 
\[
  \norm{\bp_1}^2 + \ldots + \norm{\bp_d}^2 = \sigma^2_1 + \ldots + \sigma^2_d
\]
одинаково при пошаговой и одновременной максимизации.


\end{frame}


\begin{frame}
  \frametitle{Смысл сингулярных значений}

  Квадраты сингулярных значений показывают увеличение среднего коэффициента детерминации!

%   \[
%     \begin{array}{rl}
%   \frac{\sigma^2_1}{\sigma^2_1 + \ldots + \sigma^2_k} &= \frac{1}{k} (R_1^2(\bx_1) + \ldots + R_1^2(\bx_k)) \\ \pause 
%   \frac{\sigma^2_1 + \sigma^2_2}{\sigma^2_1 + \ldots + \sigma^2_k} &= \frac{1}{k} (R_2^2(\bx_1) + \ldots + R_2^2(\bx_k)) \\ \pause
%  \frac{\sigma^2_1 + \sigma^2_2 + \sigma^2_3}{\sigma^2_1 + \ldots + \sigma^2_k} &= \frac{1}{k} (R_3^2(\bx_1) + \ldots + R_3^2(\bx_k))  
%     \end{array}
%   \]

\[
 \frac{\sigma^2_1}{\sigma^2_1 + \ldots + \sigma^2_k} = \frac{1}{k} (R_1^2(\bx_1) + \ldots + R_1^2(\bx_k))  \pause 
 \]
 \[
 \frac{\sigma^2_1 + \sigma^2_2}{\sigma^2_1 + \ldots + \sigma^2_k} = \frac{1}{k} (R_2^2(\bx_1) + \ldots + R_2^2(\bx_k)) \pause
 \]
\[
 \frac{\sigma^2_1 + \sigma^2_2 + \sigma^2_3}{\sigma^2_1 + \ldots + \sigma^2_k} = \frac{1}{k} (R_3^2(\bx_1) + \ldots + R_3^2(\bx_k))  
 \]

  

\end{frame}


% \begin{frame}
%   \frametitle{Связь с корреляционной матрицей}
%   Если $X = U\Sigma V^T$, то корреляционная матрица имеет вид $C=X^TX = V\Sigma^T \Sigma V^T$. \pause

%   Векторы весов $\bv_i$, с которыми исходные переменные входят в компоненты, являются
%   собственными векторами корреляционной матрицы $C$. \pause

%   Сингулярные значения матрицы $X$ в квадрате являются собственными числами 
%   корреляционной матрицы $C$, $\lambda_i = \sigma_i^2$.
  

% \end{frame}



\begin{frame}
  \frametitle{Резюме}

  \begin{itemize}[<+->]
  \item Жорданова нормальная форма.
  \item Сингулярное разложение.
  \item PCA: максимизация разброса.
  \item PCA: минимизация ошибки приближения.
  \item PCA: максимизация суммарного $R^2$.
  \item Скринкаст с SVD.
  \item Бонусное видео: геометрическая алгебра.
  \end{itemize}
  \pause
  
  \alert{Большое спасибо героям, прошедшим курс!}
      


\end{frame}